{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7070055d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.15.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: idna in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dell\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e728fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "#jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "#3. Then click the searchbutton.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "984d6a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries:\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de0e0849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the Selenium webdrive\n",
    "driver= webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8bed7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the Shine job portal\n",
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the search criteria\n",
    "job_titles=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]\")\n",
    "job_title.send_keys(\"data analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb66cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]\")\n",
    "job_location.send_keys(\"Bangalore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc45b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the data for the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "results = driver.find_elements_by_xpath('//div[@class=\"search_listing search_listing_new\"]')[:10]\n",
    "\n",
    "for result in results:\n",
    "    job_titles.append(result.find_element_by_xpath('.//h3').text)\n",
    "    job_locations.append(result.find_element_by_xpath('.//p[1]').text)\n",
    "    company_names.append(result.find_element_by_xpath('.//p[2]').text)\n",
    "    experience_required.append(result.find_element_by_xpath('.//p[3]').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ad2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of the scraped data\n",
    "data = {\n",
    "  'Job Title': job_titles,\n",
    "  'Job Location': job_locations,\n",
    "  'Company Name': company_names,\n",
    "  'Experience Required': experience_required}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "if df is not None:\n",
    "        print(\"Scraped Data:\")\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39a46d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a598fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ef968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "#have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.shine.com/\n",
    "#2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then scrape the data for the first 10 jobs results you get.\n",
    "#5. Finally create a dataframe of the scraped data.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70889e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the Selenium webdrive\n",
    "driver= webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7129e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the Shine job portal\n",
    "driver.get(\"https://www.shine.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de617012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enter the search criteria\n",
    "job_title=driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]\")\n",
    "job_title.send_keys(\"data scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_location=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]\")\n",
    "job_location.send_keys(\"Bangalore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37f1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "search=driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[2]\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed6d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrape the data for the first 10 job results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "results = driver.find_elements_by_xpath('//div[@class=\"search_listing search_listing_new\"]')[:10]\n",
    "\n",
    "for result in results:\n",
    "    job_titles.append(result.find_element_by_xpath('.//h3').text)\n",
    "    job_locations.append(result.find_element_by_xpath('.//p[1]').text)\n",
    "    company_names.append(result.find_element_by_xpath('.//p[2]').text)\n",
    "    experience_required.append(result.find_element_by_xpath('.//p[3]').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088ee4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of the scraped data\n",
    "data = {\n",
    "  'Job Title': job_titles,\n",
    "  'Job Location': job_locations,\n",
    "  'Company Name': company_names,\n",
    "  'Experience Required': experience_required}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "if df is not None:\n",
    "        print(\"Scraped Data:\")\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdd9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d39030",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 In this question you have to scrape data using the filters available on the webpage \n",
    "# You have to use the location and salary filter.\n",
    "#You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "#You have to scrape the job-title, job-location, company name, experience required.\n",
    "#The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "#The task will be done as shown in the below steps:\n",
    "#1. first get the web page https://www.shine.com/\n",
    "#2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "#3. Then click the search button.\n",
    "#4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "#5. Then scrape the data for the first 10 jobs results you get.\n",
    "#6. Finally create a dataframe of the scrapeddata.\n",
    "#Note: All of the above steps have to be done in code. No step is to be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "451a21bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8dfc657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.shine.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a8c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_box = driver.find_element_by_id('id_q')\n",
    "search_box.send_keys('Data Scientist')\n",
    "\n",
    "search_button = driver.find_element_by_id('id_l')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0770afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_filter = driver.find_element_by_xpath(\"//label[contains(text(),'Delhi/NCR')]\")\n",
    "location_filter.click()\n",
    "\n",
    "salary_filter = driver.find_element_by_xpath(\"//label[contains(text(),'3-6')]\")\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f2625",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles = driver.find_elements_by_xpath(\"//a[@class='job_title']\")\n",
    "job_locations = driver.find_elements_by_xpath(\"//li[@class='w-30 mr-10 result-display-location']\")\n",
    "company_names = driver.find_elements_by_xpath(\"//a[@class='result-display-company']\")\n",
    "experience_required = driver.find_elements_by_xpath(\"//li[@class='w-30 result-display-exp']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Job Title': [title.text for title in job_titles[:10]],\n",
    "  'Job Location': [location.text for location in job_locations[:10]],\n",
    "  'Company Name': [company.text for company in company_names[:10]],\n",
    "  'Experience Required': [experience.text for experience in experience_required[:10]]}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d10fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#6. Brand\n",
    "#7. ProductDescription\n",
    "#8. Price\n",
    "#To scrape the data you have to go through following steps:\n",
    "#1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "#2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and\n",
    "#click the search icon\n",
    "#3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "#required data as usual\n",
    "#4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "#click on it.\n",
    "#5. Now scrape data from this page as usual\n",
    "#6. Repeat this until you get data for 100sunglasses.\n",
    "#Note: That all of the above steps have to be done by coding only and not manually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e8399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the Selenium WebDriver for Chrome\n",
    "# Replace 'path_to_chromedriver' with the path to your chromedriver executable\n",
    "driver = webdriver.Chrome(executable_path='path_to_chromedriver')\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# Search for sunglasses on the Flipkart website\n",
    "search_items = driver.find_element(By.CLASS_NAME,\"//input[@class='_3704LK']\")\n",
    "search_items.send_keys(\"sunglasses\")\n",
    "search_btn = driver.find_element(By.CLASS_NAME,\"//button[@class='L0Z3Pu']\")\n",
    "search_btn.click()\n",
    "\n",
    "# Wait for the page to load (You may increase the sleep time if needed)\n",
    "time.sleep(5)\n",
    "\n",
    "# Define functions to scrape data from the current page\n",
    "def scrape_brand_names():\n",
    "    brand_names = []\n",
    "    brand_elements = driver.find_elements(By.CLASS_NAME,\"//div[@class='_2WkVRV']\")\n",
    "    for element in brand_elements:\n",
    "        brand_names.append(element.text)\n",
    "    return brand_names\n",
    "\n",
    "def scrape_product_descriptions():\n",
    "    product_descriptions = []\n",
    "    product_elements = driver.find_elements(By.CLASS_NAME,\"//a[@class='IRpwTa']\")\n",
    "    for element in product_elements:\n",
    "        product_descriptions.append(element.text)\n",
    "    return product_descriptions\n",
    "\n",
    "def scrape_prices():\n",
    "    prices = []\n",
    "    price_elements = driver.find_elements(By.CLASS_NAME,\"//div[@class='_30jeq3']\")\n",
    "    for element in price_elements:\n",
    "        prices.append(element.text)\n",
    "    return prices\n",
    "\n",
    "def scrape_discounts():\n",
    "    discounts = []\n",
    "    discount_elements = driver.find_elements(By.CLASS_NAME,\"//div[@class='_3Ay6Sb']/span\")\n",
    "    for element in discount_elements:\n",
    "        discounts.append(element.text)\n",
    "    return discounts\n",
    "\n",
    "# Scrape data from multiple pages until we get 100 sunglasses data\n",
    "total_sunglasses = 100\n",
    "sunglasses_data = []\n",
    "\n",
    "while len(sunglasses_data) < total_sunglasses:\n",
    "    brand_names = scrape_brand_names()\n",
    "    product_descriptions = scrape_product_descriptions()\n",
    "    prices = scrape_prices()\n",
    "    discounts = scrape_discounts()\n",
    "\n",
    "    for brand, product, price, discount in zip(brand_names, product_descriptions, prices, discounts):\n",
    "        sunglasses_data.append({'Brand': brand, 'Product Description': product, 'Price': price, 'Discount': discount})\n",
    "\n",
    "    # Click on the 'Next' button to move to the next page\n",
    "    try:\n",
    "        next_btn = driver.find_element(By.XPATH,\"//a[contains(text(),'Next')]\")\n",
    "        next_btn.click()\n",
    "        time.sleep(5)  # Wait for the next page to load\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Close the browser after scraping\n",
    "driver.quit()\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(sunglasses_data)\n",
    "\n",
    "\n",
    "if df is not None:\n",
    "        print(\"Scraped Data:\")\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3593a648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2854672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "Apple Iphone 11 Black 64 Gb Reviews: Latest Review of Apple Iphone 11 Black 64 Gb | Price in India | Flipkart.com\n",
    "\n",
    "using selenium method As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3facb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02813dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442adceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa2a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the number of scrolls based on the number of reviews you want to scrape\n",
    "num_scrolls = 10\n",
    "for _ in range(num_scrolls):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Add a delay to allow the page to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38635938",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = driver.find_elements(By.XPATH, \"//div[@class='_27M-vq']\")\n",
    "ratings = driver.find_elements(By.XPATH, \"//div[@class='_3LWZlK _1BLPMq']\")\n",
    "summaries = driver.find_elements(By.XPATH, \"//p[@class='_2-N8zT']\")\n",
    "full_reviews = driver.find_elements(By.XPATH, \"//div[@class='t-ZTKy']\")\n",
    "\n",
    "# Iterate over the reviews and extract the required data\n",
    "for i in range(100):\n",
    "    rating = ratings[i].text\n",
    "    summary = summaries[i].text\n",
    "    full_review = full_reviews[i].text\n",
    "  \n",
    "  # Process the scraped data as needed\n",
    "  # Print or store the data in a desired format\n",
    "    print(\"Review\", i+1)\n",
    "    print(\"Rating:\", rating)\n",
    "    print(\"Summary:\", summary)\n",
    "    print(\"Full Review:\", full_review)\n",
    "    print()\n",
    "\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcda6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##6) Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "#search field.\n",
    "#You have to scrape 3 attributes of each sneaker:\n",
    "#1. Brand\n",
    "#2. ProductDescription\n",
    "#3. Price\n",
    "#As shown in the below image, you have to scrape the above attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0a4b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "driver=webdriver.Chrome()\n",
    "driver.get (\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "901a70ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar=driver.find_element (\"name\", \"q\")\n",
    "search_bar.send_keys(\"sneakers\")\n",
    "search_bar.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ec0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "for _ in range(3):\n",
    "    driver.find_element(By.TAG_NAME,'body').send_keys(Keys.END)\n",
    "    time.sleep(2)\n",
    "    \n",
    "products=driver.find_element(By.CLASS_NAME,\"_1AtVbE\")\n",
    "\n",
    "for i, product in enumerate(products[:100], start =1):\n",
    "    brand =product.find_element(By.CLASS_NAME,\"_2WkVRV\").text\n",
    "    discription= product.find_element(By.CLASS_NAME,\"_IRpwTa\").text\n",
    "    price= product.find_element(By.CLASS_NAME,\"_30jeq3\").text\n",
    "    \n",
    "    print(f\"#{1} Brand: {brand}, Discription: {discription}, Price:{Price}\")\n",
    "          \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f28dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "#set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "#    After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#1. Title\n",
    "#2. Ratings\n",
    "#3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9f4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# Set the path to your webdriver (e.g., chromedriver.exe)\n",
    "#webdriver_path = '/path/to/chromedriver'\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Amazon website\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "# Locate the search bar and enter \"Laptop\"\n",
    "search_bar = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_bar.send_keys(\"Laptop\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the search results to load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"s-main-slot\")))\n",
    "\n",
    "# Set CPU Type filter to \"Intel Core i7\"\n",
    "cpu_filter = driver.find_element(By.XPATH, \"//span[text()='Intel Core i7']\")\n",
    "cpu_filter.click()\n",
    "\n",
    "# Wait for the filter to be applied\n",
    "time.sleep(3)\n",
    "\n",
    "# Find the laptops\n",
    "laptops = driver.find_elements(By.CSS_SELECTOR, \".s-result-item\")\n",
    "\n",
    "# Scrape data for the first 10 laptops\n",
    "for i, laptop in enumerate(laptops[:10], start=1):\n",
    "    title = laptop.find_element(By.CSS_SELECTOR, \".s-title-instructions a span\").text\n",
    "    try:\n",
    "        ratings = laptop.find_element(By.CSS_SELECTOR, \".s-asin .a-icon-star-small .a-icon-alt\").text\n",
    "    except:\n",
    "        ratings = \"No ratings available\"\n",
    "    try:\n",
    "        price = laptop.find_element(By.CSS_SELECTOR, \".s-price .a-offscreen\").text\n",
    "    except:\n",
    "        price = \"Price not available\"\n",
    "\n",
    "    print(f\"#{i} Title: {title}\\n   Ratings: {ratings}\\n   Price: {price}\\n\")\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5979c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "#The above task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.azquotes.com/\n",
    "#2. Click on TopQuoteS\n",
    "#3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5fdc14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver= webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "top_quotes_button = driver.find_element(By.XPATH, \"//a[contains(text(),'Top Quotes')]\")\n",
    "top_quotes_button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape data for the Top 1000 Quotes\n",
    "quotes = driver.find_elements(By.XPATH, \"//div[@class='wrap-block']/div[@class='block']\")\n",
    "\n",
    "# Scrape data for the first 10 quotes (you can change this to 1000)\n",
    "for i, quote in enumerate(quotes[:10], start=1):\n",
    "    quote_text = quote.find_element(By.CLASS_NAME, \"title\").text\n",
    "    author = quote.find_element(By.CLASS_NAME, \"author\").text\n",
    "    quote_type = quote.find_element(By.CLASS_NAME, \"kw-box\").text\n",
    "\n",
    "    print(f\"#{i} Quote: {quote_text}\\n   Author: {author}\\n   Type: {quote_type}\\n\")\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90473da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "#Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpagehttps://www.jagranjosh.com/\n",
    "#2. Then You have to click on the GK option\n",
    "#3. Then click on the List of all Prime Ministers of India\n",
    "#4. Then scrap the mentioned data and make theDataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "361b318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()\n",
    "driver.get (\"https://www.jagranjosh.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_option=driver.find_element(By.XPATH,\"//a[contains(text(),'GK')]\")\n",
    "gk_option.click()\n",
    "\n",
    "PM_option=driver.find_element(By.XPATH,\"//a[contains(text(),'List of all Prime Ministers of India')]\")\n",
    "PM_option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d97d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "table =driver.find_element(By.CLASS_NAME,\"table\")\n",
    "rows =table.find_element(By.TAG_NAME,\"tr\")\n",
    "for roe in rows:\n",
    "    cells=row.find_element(By.TAG_NAME,\"td\")\n",
    "    if len cells==4:\n",
    "        name=cells[0].text\n",
    "        born_dead=cells[1].text\n",
    "        term_of_office=cells[2].text\n",
    "        remarks=cells[3].text\n",
    "        data.append(data, column=[name,born_dead,term_of_office,remakrs])\n",
    "        \n",
    "df=pd.Dataframe(data, columns=['Name','Born_Dead','Term_of_Office','Remarks'])\n",
    "print(df)\n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aadee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "#Car name and Price) from https://www.motor1.com/\n",
    "#This task will be done in following steps:\n",
    "#1. First get the webpage https://www.motor1.com/\n",
    "#2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "#3. Then click on 50 most expensive carsin the world..\n",
    "#4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f686d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver chrome()\n",
    "driver.get (\"https://www.motor1.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar=driver.find_element(By.NAME,\"q\")\n",
    "search_bar.send.keys(\"50 most expensive cars\")\n",
    "search_bar.send.keys(Keys.RETURN)\n",
    "driver.implicitly_wait(5)\n",
    "page_source=driver.page_source\n",
    "soup=BeautifulSoup(page_source,'html parser')\n",
    "\n",
    "cars_data = []\n",
    "for car in soup.find_all('div', class_='motor-loan-carList-group__item'):\n",
    "    car_name = car.find('h3').text.strip()\n",
    "    car_price = car.find('p', class_='motor-loan-carList-group__price').text.strip()\n",
    "\n",
    "    cars_data.append([car_name, car_price])\n",
    "    \n",
    "df=pd.DataFrame(cars_data, column=['Car Name','Price'])\n",
    "print (df)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c811b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a659fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1baca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cb7a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d88e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dca916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
