{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38c1daac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "47aa67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc59c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vyanky= requests.get('https://en.wikipedia.org/wiki/Main_Page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e690f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vyanky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "d2750f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a4bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##1Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c3de841",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "response=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3223d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup=BeautifulSoup(response.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bf2d462e",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_tag= soup.find_all(['h1','h2','h3','h4','h5','h6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d27e48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [tag.get_text() for tag in page_tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "79355dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Headers Text\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "df= pd.DataFrame(text, columns=[\"Headers Text\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc32db70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "#from https://presidentofindia.nic.in/former-presidents.htm and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "54302bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "url='https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "president_data=[]\n",
    "table = soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows= table.find_all('tr')[1:]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        if len(columns)>=2:\n",
    "            names=columns[0].text.strop()\n",
    "            period=columns[1].text.strip()\n",
    "            president_data.append({'Name':names, 'Term of office':period})\n",
    "            \n",
    "df= pd.DataFrame(president_data)\n",
    "print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f608b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "#c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "9f45323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Team Name Matches Points rating\n",
      "0        India\\nIND      55  6,640    121\n",
      "1    Australia\\nAUS      42  4,926    117\n",
      "2  South Africa\\nSA      34  3,750    110\n",
      "3     Pakistan\\nPAK      36  3,922    109\n",
      "4   New Zealand\\nNZ      43  4,399    102\n",
      "5      England\\nENG      38  3,777     99\n",
      "6     Sri Lanka\\nSL      47  4,134     88\n",
      "7   Bangladesh\\nBAN      44  3,836     87\n",
      "8  Afghanistan\\nAFG      30  2,533     84\n",
      "9   West Indies\\nWI      38  2,582     68\n"
     ]
    }
   ],
   "source": [
    "url='https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "odi_data=[]\n",
    "table = soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows= table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        if len(columns)>=2:\n",
    "            team_name=columns[1].text.strip()\n",
    "            matches=columns[2].text.strip()\n",
    "            point=columns[3].text.strip()\n",
    "            rating=columns[4].text.strip()\n",
    "            \n",
    "            \n",
    "            odi_data.append({'Team Name':team_name, 'Matches':matches, 'Points': point, 'rating': rating})\n",
    "            \n",
    "df= pd.DataFrame(odi_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0e77aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Top 10 ODI Batsmen along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "81783679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player Name Team Name Rating\n",
      "0           Shubman Gill       IND    826\n",
      "1             Babar Azam       PAK    824\n",
      "2            Virat Kohli       IND    791\n",
      "3           Rohit Sharma       IND    769\n",
      "4        Quinton de Kock        SA    760\n",
      "5         Daryl Mitchell        NZ    750\n",
      "6           David Warner       AUS    745\n",
      "7  Rassie van der Dussen        SA    735\n",
      "8           Harry Tector       IRE    729\n",
      "9            Dawid Malan       ENG    729\n"
     ]
    }
   ],
   "source": [
    "url='https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "top_batting=[]\n",
    "table=soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows=table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        player=columns[1].text.strip()\n",
    "        team=columns[2].text.strip()\n",
    "        rating=columns[3].text.strip()\n",
    "        \n",
    "        top_batting.append({'Player Name': player, 'Team Name': team, 'Rating': rating})\n",
    "\n",
    "df=pd.DataFrame(top_batting)\n",
    "print(df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f3f353",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3364af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Player Name Team Name Rating\n",
      "0  Keshav Maharaj        SA    741\n",
      "1  Josh Hazlewood       AUS    703\n",
      "2  Mohammed Siraj       IND    699\n",
      "3  Jasprit Bumrah       IND    685\n",
      "4      Adam Zampa       AUS    675\n",
      "5     Rashid Khan       AFG    667\n",
      "6   Kuldeep Yadav       IND    667\n",
      "7     Trent Boult        NZ    663\n",
      "8  Shaheen Afridi       PAK    650\n",
      "9  Mohammad Shami       IND    648\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "bowler_data=[]\n",
    "table=soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows=table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        player=columns[1].text.strip()\n",
    "        team=columns[2].text.strip()\n",
    "        rating=columns[3].text.strip()\n",
    "        \n",
    "        bowler_data.append({'Player Name': player, 'Team Name': team, 'Rating': rating})\n",
    "        \n",
    "df=pd.DataFrame(bowler_data)\n",
    "print(df)       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f9fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c92b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dbb7d0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Team Name Matches Points rating\n",
      "0    Australia\\nAUS      21  3,429    163\n",
      "1      England\\nENG      23  2,991    130\n",
      "2  South Africa\\nSA      21  2,446    116\n",
      "3        India\\nIND      18  1,745     97\n",
      "4   New Zealand\\nNZ      21  2,014     96\n",
      "5   West Indies\\nWI      20  1,768     88\n",
      "6     Sri Lanka\\nSL       9    714     79\n",
      "7   Bangladesh\\nBAN      14  1,074     77\n",
      "8     Thailand\\nTHA      11    753     68\n",
      "9     Pakistan\\nPAK      24  1,602     67\n"
     ]
    }
   ],
   "source": [
    "url='https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.content,\"html.parser\")\n",
    "\n",
    "odi_data=[]\n",
    "table = soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows= table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        if len(columns)>=2:\n",
    "            team_name=columns[1].text.strip()\n",
    "            matches=columns[2].text.strip()\n",
    "            point=columns[3].text.strip()\n",
    "            rating=columns[4].text.strip()\n",
    "            \n",
    "            \n",
    "            odi_data.append({'Team Name':team_name, 'Matches':matches, 'Points': point, 'rating': rating})\n",
    "            \n",
    "df= pd.DataFrame(odi_data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240fe436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7a741a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Player Name Team Name Rating\n",
      "0  Natalie Sciver-Brunt       ENG    807\n",
      "1           Beth Mooney       AUS    750\n",
      "2   Chamari Athapaththu        SL    736\n",
      "3       Laura Wolvaardt        SA    727\n",
      "4       Smriti Mandhana       IND    708\n",
      "5          Alyssa Healy       AUS    698\n",
      "6          Ellyse Perry       AUS    697\n",
      "7      Harmanpreet Kaur       IND    694\n",
      "8           Meg Lanning       AUS    662\n",
      "9        Marizanne Kapp        SA    642\n"
     ]
    }
   ],
   "source": [
    "url='https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "top_batting=[]\n",
    "table=soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows=table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        player=columns[1].text.strip()\n",
    "        team=columns[2].text.strip()\n",
    "        rating=columns[3].text.strip()\n",
    "        \n",
    "        top_batting.append({'Player Name': player, 'Team Name': team, 'Rating': rating})\n",
    "\n",
    "df=pd.DataFrame(top_batting)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecfd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b85860cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Player Name Team Name Rating\n",
      "0        Marizanne Kapp        SA    385\n",
      "1      Ashleigh Gardner       AUS    377\n",
      "2  Natalie Sciver-Brunt       ENG    360\n",
      "3       Hayley Matthews        WI    358\n",
      "4           Amelia Kerr        NZ    346\n",
      "5         Deepti Sharma       IND    312\n",
      "6          Ellyse Perry       AUS    282\n",
      "7              Nida Dar       PAK    240\n",
      "8         Jess Jonassen       AUS    227\n",
      "9         Sophie Devine        NZ    227\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "allrounder_data=[]\n",
    "table=soup.find('table',{'class':'table'})\n",
    "\n",
    "if table:\n",
    "    rows=table.find_all('tr')[1:11]\n",
    "    for row in rows:\n",
    "        columns=row.find_all('td')\n",
    "        player=columns[1].text.strip()\n",
    "        team=columns[2].text.strip()\n",
    "        rating=columns[3].text.strip()\n",
    "        \n",
    "        allrounder_data.append({'Player Name': player, 'Team Name': team, 'Rating': rating})\n",
    "        \n",
    "df=pd.DataFrame(allrounder_data)\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002i) Headline\n",
    "ii) Time\n",
    "iii) News LinK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "023cc1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Headlines  Time  \\\n",
      "0   Stocks making the biggest moves premarket: Pfi...  None   \n",
      "1   Dow futures start December flat after average ...  None   \n",
      "2   Stocks making the biggest moves premarket: Pfi...  None   \n",
      "3   UAE seeks to silence critics with $30-billion ...  None   \n",
      "4   Bill Gates warns the world is likely to oversh...  None   \n",
      "5   Bill Gates shares his ‘big hope’ for the COP28...  None   \n",
      "6   Countries at COP28 approve climate disaster fu...  None   \n",
      "7   Bill Gates: I have hope in messages coming out...  None   \n",
      "8   Op-ed: With continued geopolitical conflicts, ...  None   \n",
      "9   Ukraine war live updates: Zelenskyy calls for ...  None   \n",
      "10  Multiple civilians injured in strikes on Ukrai...  None   \n",
      "11  Russia slams Finland's border closure, warns t...  None   \n",
      "12  Finland to close entire border with Russia; wi...  None   \n",
      "13               CNBC's Sustainable Future Forum 2023  None   \n",
      "14  Wind power industry in moment of reckoning as ...  None   \n",
      "15  CEO on why natural gas infrastructure must be ...  None   \n",
      "16  Why one Tesla manager thinks used cars are 'ab...  None   \n",
      "17  Volvo Cars CEO strikes cautious tone on solid-...  None   \n",
      "18  Southeast Asia is on the cusp of a 'supercharg...  None   \n",
      "19  Cambodia's deputy prime minister: BRI has help...  None   \n",
      "20  Southeast Asia's first luxury hotel made from ...  None   \n",
      "21  Ahead of Indonesia’s elections, critics slam J...  None   \n",
      "22  Laos is spiraling toward a debt crisis as Chin...  None   \n",
      "23  Pack your jerseys. The era of 'sports tourism'...  None   \n",
      "24  See the photos that made National Geographic's...  None   \n",
      "25  The ultimate work perk? This company provides ...  None   \n",
      "26  Fear is driving Chinese travelers away from tw...  None   \n",
      "27  A UNESCO World Heritage site with thousands of...  None   \n",
      "28  Your network is the key to your career, says t...  None   \n",
      "29  Parenting tips from a dad who’s fostered 36 ki...  None   \n",
      "30  6 tips to really disconnect from work for the ...  None   \n",
      "31  Study: Anxious dads raise smarter, more well-b...  None   \n",
      "32  On ChatGPT's one-year anniversary, it has more...  None   \n",
      "\n",
      "                                            News link  \n",
      "0   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "1   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "2   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "3   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "4   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "5   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "6   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "7   https://www.cnbc.com/https://www.cnbc.com/vide...  \n",
      "8   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "9   https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "10  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "11  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "12  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "13  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "14  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "15  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "16  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "17  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "18  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "19  https://www.cnbc.com/https://www.cnbc.com/vide...  \n",
      "20  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "21  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "22  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "23  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "24  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "25  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "26  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "27  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "28  https://www.cnbc.com/https://www.cnbc.com/vide...  \n",
      "29  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "30  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "31  https://www.cnbc.com/https://www.cnbc.com/2023...  \n",
      "32  https://www.cnbc.com/https://www.cnbc.com/2023...  \n"
     ]
    }
   ],
   "source": [
    "url='https://www.cnbc.com/world/?region=world'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "news_data=[]\n",
    "articles=soup.find_all ('div',class_=\"Card-titleContainer\")\n",
    "for article in articles:\n",
    "    headline= article.find('a').text.strip()\n",
    "    timestamp_element= article.find('time')\n",
    "    timestamp= timestamp_element['data-time'] if timestamp_element else None\n",
    "    news_link=\"https://www.cnbc.com/\"+article.find('a')['href']\n",
    "    \n",
    "    news_data.append({'Headlines':headline, 'Time':timestamp,'News Link':news_link})\n",
    "\n",
    "df=pd.DataFrame(news_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c4beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "#days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "#Scrape below mentioned details and make data frame\u0002i) Paper Title\n",
    "#ii) Authors\n",
    "#iii) Published Date\n",
    "#iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b31f6d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "articles_container=soup.find(\"div\", class_='pod_listing')\n",
    "\n",
    "if articles_container:\n",
    "    titles=[]\n",
    "    authors=[]\n",
    "    dates=[]  \n",
    "    urls=[]\n",
    "\n",
    "    for article in articles_container.find_all('li'):\n",
    "        title= article.find('h2').text.strip()\n",
    "        author= article.find('Span', class_=\"author_list\" ).text.strip()\n",
    "        date= article.find('Span', class_=\"pod-listing-date\" ).text.strip()\n",
    "        url= article.find(\"a\")['href']\n",
    "\n",
    "        titles.append(title)\n",
    "        authos.append(author)\n",
    "        dates.append(date)\n",
    "        urls.append(urls)\n",
    "\n",
    "        data={'Tiles': titles,'Author':Authors,'Date': Dates, 'urls':urls}\n",
    "df=pd.DataFrame(data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7)Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant name\n",
    "#ii) Cuisine\n",
    "#iii) Location\n",
    "#iv) Ratings\n",
    "#v) Image UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd0a8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "407c9745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, cuisine, Location, Rating, Image Url]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "url=\"https://www.dineout.co.in\"\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "restaurant_name=soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines= soup.find_all('span', class_='doubble-line ellipsis')\n",
    "locations= soup.find_all('span', class_='doubble-line ellipsis')\n",
    "ratings = soup.find_all('div', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "\n",
    "for name in restaurant_name:\n",
    "    restaurant_list.append(name.text.strip())\n",
    "    \n",
    "for cuisine in cuisines:\n",
    "    cuisine_list.append(cuisine.text.strip())\n",
    "        \n",
    "for location in locations:\n",
    "    location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "    rating_list.append(rating.text.strip())\n",
    "        \n",
    "for image in image_urls:\n",
    "    image_url_list.append(image['data-src'])\n",
    "    \n",
    "Data={'Restaurant Name': restaurant_list,'cuisine' : cuisine_list,'Location':location_list,'Rating':rating_list,'Image Url':image_url_list}\n",
    "\n",
    "df=pd.DataFrame(Data)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3c09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612fca77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc974b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee84f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325cf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "47d19ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the elements containing the details you want to scrape\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "\n",
    "# Create empty lists to store the scraped data\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "# Extract the data from the elements and append them to the respective lists\n",
    "for name in restaurant_names:\n",
    "    restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "    cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "    location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "    rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "    image_url_list.append(image['src'])\n",
    "\n",
    "# Create a dictionary from the lists\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "# Create a dataframe from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "647151cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df879e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
